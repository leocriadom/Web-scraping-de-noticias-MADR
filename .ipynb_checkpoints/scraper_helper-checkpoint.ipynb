{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autor: Leonel Criado\n",
    "\n",
    "Proyecto: Análisis de noticias del Ministerio de Agricultura y Desarrollo Rural de Colombia\n",
    "\n",
    "Creado: 2020-05-18\n",
    "\n",
    "Meta: Funciones de ayuda para web scraping de noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necesitamos:\n",
    "- Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(url):\n",
    "    '''\n",
    "    Scraping de las noticias\n",
    "    '''\n",
    "    resp = requests.get(url).text\n",
    "    return BeautifulSoup(resp,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(soup):\n",
    "    '''\n",
    "    Retorna el texto de la noticia\n",
    "    '''\n",
    "    if str(soup.find_all(['p', 'div'], {\"class\": \"pad\"}))!=\"[]\":\n",
    "        soup = str(soup.find_all(['p', 'div'], {\"class\": \"pad\"}))\n",
    "    elif str(soup.find_all(['p', 'div'], {\"class\": \"article-content\"}))!=\"[]\":\n",
    "        soup = str(soup.find_all(['p', 'div'], {\"class\": \"article-content\"}))\n",
    "    else:\n",
    "        soup = str(soup.find_all(['p', 'div'], {\"class\": \"contenido grd9\"}))\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(soup):\n",
    "    '''\n",
    "    Returna el título de la noticia\n",
    "    '''\n",
    "    try:\n",
    "        title = soup.title.text.strip()\n",
    "    except AttributeError:\n",
    "        title = \"N.A\"\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(soup):\n",
    "    '''\n",
    "    Retorna la fecha de la noticia\n",
    "    '''\n",
    "    date = str(soup.find_all(\"span\",class_=\"fecha\"))\n",
    "    if len(re.findall('\\xa0(.{10})', date)) != 0:\n",
    "        date = re.findall('\\xa0(.{10})', date)[0]\n",
    "        date = date.split(\"/\")\n",
    "        dia = date[0]\n",
    "        mes = date[1]\n",
    "        año = date[2]\n",
    "        año = re.findall('[0-9]+', año)\n",
    "    else:\n",
    "        dia = 'N.A.'\n",
    "        mes = 'N.A.'\n",
    "        año = 'N.A.'       \n",
    "    return [dia, mes, año]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(soup):\n",
    "    '''\n",
    "    Retorna la ubicación\n",
    "    '''\n",
    "    text = str(soup.find_all(['p', 'div'], {\"class\": \"pad\"}))\n",
    "    if len(re.findall('strong>(.*)</strong', text)) != 0:\n",
    "        ubicacion = re.findall('strong>(.*)</strong', text)[0]\n",
    "        if len(re.findall(\"(\\w+),\", ubicacion)) != 0:\n",
    "            ubicacion = re.findall(\"(\\w+),\", ubicacion)[0]\n",
    "        else:\n",
    "            ubicacion = 'N.A.'\n",
    "    elif len(re.findall('div><div><br/></div><div>(.*) –\\xa0', text)) != 0:\n",
    "        ubicacion = re.findall('div><div><br/></div><div>(.*) –\\xa0', text)[0]\n",
    "        ubicacion = re.findall(\"(\\w+),\", ubicacion)[0]\n",
    "    else:\n",
    "        ubicacion = 'N.A.'\n",
    "    return ubicacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "    '''\n",
    "    Limpia el texto de la noticia y obtiene la ubicación\n",
    "    '''\n",
    "    news = re.sub('<[^>]+>|\\]|\\[|\\r|\\t|\\\\n|\\xa0|\\\\u200b', ' ', text) #•\n",
    "    news = re.sub(\"Contenido de la página \",\"\", news)\n",
    "    news = re.sub(\"Red de Comunicaciones\",\"\\t\", news)\n",
    "    news = re.sub(\"Leyenda de imagen\",\"\\t\", news)\n",
    "    seccion = news.count(\"\\t\")\n",
    "    if seccion==2:\n",
    "        news = re.search('\\t(.*)\\t', news).group()\n",
    "        news = re.sub('|\\t', '', news)\n",
    "    elif seccion==1:\n",
    "        news = re.search('\\t(.*)', news).group()\n",
    "        news = re.sub('|\\t', '', news)\n",
    "    news = \" \".join(news.split())\n",
    "    return [news, seccion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_elements(news_url):\n",
    "    '''\n",
    "    Retorna los elementos de la noticia\n",
    "    '''\n",
    "    print('News url: ', news_url)\n",
    "    soup = get_html(news_url)\n",
    "    title = get_title(soup)\n",
    "    dia, mes, año = get_date(soup)\n",
    "    ubicacion = get_location(soup)\n",
    "    news_MADR, seccion = remove_tags(get_news(soup))\n",
    "    time.sleep(random.randint(1,5))\n",
    "    return [dia, mes, año, ubicacion, title, news_url, news_MADR, seccion]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
